# Rubrix Copilot Instructions

- Stack: React + Tailwind UI in [src](src), Tauri/Rust backend in [src-tauri/src](src-tauri/src) with Bedrock LLM client and IMS QTI export.
- Front controller [src/App.tsx](src/App.tsx): loads subjects/topics via Tauri `invoke`, streams generation events (`llm-stream`) into the preview pane, stores questions in local state, and drives tabs (generator vs bank editor).
- Commands exposed in [src-tauri/src/main.rs](src-tauri/src/main.rs): `get_subjects`, `get_topics`, `generate_questions`, `regenerate_question`, CRUD for questions, exports (`export_to_txt`, `export_to_qti`, `export_to_docx`), and question-bank IO (`load_question_bank`, `save_question_bank`). Add frontend invokes plus handler wiring when adding a new command.
- Streaming: `llm::generate` emits partial text via `emit_all("llm-stream")`; UI listens in App and shows raw buffer in [src/components/StreamingPreview.tsx](src/components/StreamingPreview.tsx). Keep event name/payload `{text, done}` stable.
- LLM client [src-tauri/src/llm.rs](src-tauri/src/llm.rs): calls Bedrock `openai.gpt-oss-120b-1:0` with bearer token `AWS_BEARER_TOKEN_BEDROCK`; falls back to deterministic mock streaming when missing. Logs prompt/response to ../llm_log.txt.
- Prompt building/parsing lives in [src-tauri/src/prompts.rs](src-tauri/src/prompts.rs) with JSON-array contract for questions; regeneration uses existing question + context to avoid duplicates.
- Knowledge base loader [src-tauri/src/knowledge.rs](src-tauri/src/knowledge.rs): embeds default assets from [imports/knowledge](imports/knowledge) via rust-embed. Overrides when `RUBRIX_KNOWLEDGE_DIR` is set (absolute or relative). Topics/subtopics and question-bank.json drive prompts and bank editor.
- Bank editor UI [src/components/BankEditor.tsx](src/components/BankEditor.tsx) loads/saves banks per subject through Tauri commands and writes JSON atomically to the knowledge dir determined by `knowledge_base_dir`.
- Question display/editor: [src/components/QuestionCard.tsx](src/components/QuestionCard.tsx) renders markdown+LaTeX+code, normalizes `\(...\)`→`$...$` and formats step headings; [src/components/EditModal.tsx](src/components/EditModal.tsx) enforces at least one correct answer.
- QTI export [src-tauri/src/qti.rs](src-tauri/src/qti.rs): cleans UTF-8 artifacts, converts markdown tables/LaTeX to HTML or remote SVG images, and zips manifest + XML. TXT export orders correct answers first; DOCX export posts markdown to an external converter API.
- Types shared with UI are in [src/types.ts](src/types.ts); backend mirrors them. Keep fields aligned (e.g., `answers` array with `is_correct`).
- UI state assumptions: questions are numbered `q1`, `q2`, …; append mode appends with renumbering; sessions saved/opened as JSON use `{questions: [...]} or raw array` handled by `parseSessionQuestions` in App.
- Build/run: `npm install`; `npm run tauri dev` for full app; `npm run dev` for frontend only; backend alone with `cd src-tauri && cargo build`. Release uses `npm run tauri build`.
- Versioning: `npm version <x>` triggers `npm run version` script to sync `package.json` version into [src-tauri/tauri.conf.json](src-tauri/tauri.conf.json) and stages the file.
- Styling/rendering: Markdown rendered with ReactMarkdown + remark-math/rehype-katex; code blocks use Prism theme; keep math delimiters consistent and prefer markdown over raw HTML when possible.
- When touching knowledge assets or prompts, update both bundled files under imports/knowledge and ensure the loader can find overrides; avoid breaking topic IDs expected by question-bank.json.
- Before adding new env/config, consider Tauri packaging: `.env` is loaded in llm/main during runtime and not bundled by default.
- Regression risks: changes to answer ordering affect QTI correctness mapping; adjustments to streaming payload shape require frontend updates; modifying knowledge dir resolution impacts bank editing persistence.
- Typical debug flow: watch streamed buffer to see raw LLM output; logs are appended to llm_log.txt (repo root parent from src-tauri). Use mock mode by omitting AWS token to reproduce streams without network.
